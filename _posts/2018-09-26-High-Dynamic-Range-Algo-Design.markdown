---
layout:     post
title:      "High Dynamic Range Algorithm Design"
subtitle:   " \"ISP Design Part I\""
date:       2018-09-26 12:00:00
author:     "McCreeNing"
header-img: "img/HDR.jpeg"
tags:
    - 算法
    - 编程
    - 科普
---

# 前言

  HighDynamicRange顾名思义，叫做高动态范围技术，是一种用一张图片在低动态显示器上可以同时显示极亮极暗处细节的一种图像处理技术。

  初步估计本文会包含：

  - HDR技术的基本概念
  - 实现过程中遇到的难点和工程化技巧
  - 各模块实现功能区分
  - 最终算法DEMO的相关参数
  - 后续可能的改进方向以及硬件化实现讨论

本文将跟随项目实际进度持续更新，欢迎大家提出宝贵意见，本人：McCreeNing 编程功力和硬件开销的了解不够深入，因此希望跟大家一起进步。

本文仅仅作为简单的补充说明，因此全篇不会有多余引用和其他参照，仅仅凭借个人项目经验进行总结，因此如有疏漏请及时与我沟通。

# HDR基本概念

生活中我们用眼睛看到的场景，偶尔会出现从暗处走向光明觉得刺眼无法适应，需要花一点时间才能正视外面的天空，对于外面这种太阳高照的场景如果我们用七大物理常量之一的光强度坎德拉来形容，就是在触目可及的场景中又有光强很高很高的太阳，又有很暗的树荫，甚至于对于坎德拉而言这二者的光强完全不是一个普通的数量级。

而目前我们用来显示图像的显示器们，比如电脑屏幕电视屏幕或者印刷品上面的图片等等，如果大家好奇可以打开windows的画图功能，通过调整一下画板的RGB值就可以知道，我们所最常用的图片显示装置都是基于8bit，也就是最大值只有256而已。在我们没法使用小数的情况下，我们需要使用0-255这256个灰阶来表示全部的亮度信息【无论是亮度还是照度，他们在实际场景中都是远远高于8bit/256个取值范围的】，因此我们需要使用一种技术能够把太阳+树荫这种高动态范围的亮度场景在同一个低位宽显示装置上显示出来，并且可以让观者能够不影响观感的清晰看到亮处以及暗处的全部细节。使用普通充满限制的灰阶来显示大量的高动态范围的数据，这就是我们HDR算法所要实现的功能。

# HDR模块功能区分

---
        2018年10月10日15:16:45 update
---

因为要准备晚上给组员们的分享会，要教教他们HDR算法里面到底做了什么东西，所以赶紧趁着这个机会把这篇博文填点坑出来。晚上讲东西就按照这个讲了。

### 综述

为了能够同时拥有高亮和阴暗场景两部分的细节主体，因此仅仅通过单一sensor一重曝光很难得到让我们满意的图像（不然要我们HDR干嘛），因此我们通过使用同一sensor进行多重曝光，通过控制曝光行曝光增益等曝光相关参数，得到原图覆盖部分亮度区域细节的多张不同曝光原图。而我们也正是基于这几张图进行我们HDR算法的展开运算，将每个图中所需要的清晰细节进行分割集合成同一张图进行最终的输出。

基于HDR的基本概念，我们需要有两部分主体模块，首先是用来将多张拥有差异图像细节的原图根据某种规则合成为同一张图。为了让两张不同曝光（理论上是增益不同的两张图）进行同一化合成，因此我们得到的图位宽将大大高于原图（正相关于几张图的曝光比）。这部分合成模块我们称之为Combine。

为了让这张HDR图能够在LDR显示器256的的范围内得以正确显示并保留HDR的效果，因为我们还需要一次压缩将高动态的数据压缩成8bit的数据量。然而对于一张有极亮有极暗的图像，按照韦伯定律的习惯性总结，人们在意的往往都是暗处的细节，毕竟亮了之后都是白白白白你能看到啥，因此我们无法简单的进行线性压缩，我们要在保留高亮细节区分的程度之外要尽可能的提亮暗处并保留全部细节，因此我们同样需要找到一个映射关系作为我们进行压缩的规则，这部分功能模块我们称之为ToneMapping。

为了把我们的算法做成模块化的算法，因此我们希望它拥有足够的鲁棒性跟适配性，有充分的后续升级可能，因此我们把这几部分代码均抽象成单一固定接口，仅仅传入固定的数据流已经需要初始化的相关参数，抽象成相关接口方便硬件化模块化完成算法的移植工作。

而对于芯片而言，为了使得tonemapping模块可以复用在多种比特位宽的输入中，因此我们定义了一个接口层模块：normalize。其主要的效果是将combine的输出数据不达到16bit的时候根据当前bit位数统一固定位宽到16bit交由tonemapping进行处理。而tonemapping贼固定的根据输入位宽进行HDR-LDR的压缩工作。

其他剩余模块主要功能是在不同的数域进行对亮度的处理，主要为了提升暗处细节的亮度，对高亮部分占用过多的情况进行压缩。面对可能存在的饱和度下降以及可能出现的比特位宽溢出情况进行我们所需要的限制和压缩。


# HDR分模块详解

首先该算法是ISP芯片内置算法，因此及其追求硬件友好型，代码设计风格上也十分适配硬件设计语言，为了方便硬件实现，因此很多参数、接口、内存空间管理均以方便硬件实现为准，不过后续在我重构当前算法时，会尽量考虑软件性能开销。

## 主函数

作为一个通用的模块，因此需要满足硬件流水线上对于每一个模块task化的要求，所有函数的参数以及相关初始化需要进行分割，为了方便后续测试人员调试具体产品，因此也希望我们各对象接口的可调性足够强，因此会抽象出很多函数内函数外参数作为调试专用。

其中主函数便提供了由命令行传参到main函数argv[]的功能，内部通过strcmp对应传参得到具体本次编译执行过程中需要使用哪部分的模块。其中也包括了待处理图像的长宽地址等信息。

提供一个单一接口作为HDR的同一接口，主函数主要包括：
- 从命令行传参进函数主题并进行存储
- 读取外部一个参数.txt文件作为函数内部寄存器初始化参数默认值
- 初始化全局结构体作为外界配置时候的初始化寄存器参数
- 加载HDR输入图像数据到一个首地址int*的内存中
- 执行HDR算法的通用接口
- 储存HDR算法输出到本地（实际硬件是通过pipeline将数据流传递给下一个模块）

## 初始化函数

---

2018年12月7日14点57分更新

---


HDR算法的初始化主要包括两部分：1.从执行可执行文件同时读取命令行传参比如输入图像绝对路径/图像尺寸/各模块选择器使能开关，以及通过参数文本文件读入的函数内部需要使用的算法参数初始值。2.一些从外界读取比较复杂，以及部分用于firmware使用的多帧运算可变参数，在内部算法开始时进行一次算法内部赋值初始化。

前者在硬件实现上一般是有pipeline前序模块传递相关信息，比如：pixel order/bit witdh/image相关参数信息等，以及一些硬件寄存器RW/RO可以供调试人员使用。而后者一般局限于固件寄存器以及部分封装内部接口不需要暴露给外界使用。

原则上，设计算法过程中为了保证全局各模块参数传递以及多帧固件流程，采用多个全局结构体进行参数存储和传递。我们需要注意对全局结构体声明/初始化/以及范围检查等操作，保证各模块使用的数据有效且可靠。


## 顶层函数

---

2018年12月26日15点14分更新

---

顶层函数主要为了提供给执行HDR算法的通用接口，同时为了视频模拟进行了对重复帧输入输出的模拟操作。在HDR算法结束后，针对RAW12的输出进行了一次CIP插值将RAW数据转化为RGB8bit的数据输出方便进行图像效果比对。

## HDR算法处理函数

HDR算法处理函数作为HDR各模块的接口封装层，主要是调用了HDR算法内部的各个具体模块，并统一为同一个接口供顶层函数调用，其中需要的输入主要是在初始化函数中进行初始化的全局参数用来给函数内部使用，以及需要处理的数据输入本算法使用三张不同曝光值的图像数据作为图像输入，以及其他const类型参数。

HDR算法处理函数特异化操作主要在于为了支持硬件设计，做了一些硬件友好的优化：

- 重复的开辟了很多内存空间，为了区分各子模块的输入输出，在硬件实现过程中不涉及到固定地址的内存读取写入操作，因此通过不一样的变量名加以定义可以使得代码阅读性更好。

- 提供了各个子模块的使能开关，根据全局开关参数判断当前HDR算法流程应该bypass掉哪部分，根据不同的使能开关组合，HDR算法提供了WDR/两帧HDR/三帧HDR等数据通路，并提供了多个效果模块GAMMA/饱和度处理等。

---

## HDR算法子模块

### 解压缩模块

正常HDR算法的输入是由HDRsensor时分复用提供两张到三张不同曝光的同样大小同一场景原图经过MIPI特殊协议传输并在HDR算法内部进行combine过程输出高动态图像进行ToneMapping进行压缩。而市场上存在一些sensor本身在sensor内部就有combine模块，在传输到ISP之前便经过自己sensor内部的combine模块生成一个高动态范围的图像，而为了满足图像输出格式要求，以及减小图像位宽的需求，sensor内部一般进行了一次非线性压缩再传输。

因此对于这类sensor，我们需要在获得数据的最开始进行一次数据解压缩获得原始高动态图像后直接送到tonemapping入口处进行处理。

算法整体思路是根据sensor自带压缩模块的压缩公式进行方向推导，通过一个跟节点相关的矩阵进行函数反变换。

### 场景检测模块

实际上，对于这版HDR算法实际应用产品场景并不固定，可能用于行车记录仪/安防/或者其他产品上，那么对于不同场景我们提出了不一样的HDR效果需求。简单的抽象以后，我们需要检测当前场景是白天（高照度）或是夜晚（低照度），针对不一样的场景，我们需要对后续模块的强弱以及功能进行调整以满足需求。比如相比较白天，夜晚场景合成过程中噪点变大，短图出现了很多无效信息，因此需要改变合成权重，gamma模块也需要进行适当的加强以提亮暗处亮度，而ToneMapping模块对于限制对比度的阈值也要进行匹配的修改等。

算法整体的思路是针对多光源，复杂场景，亮暗场景也应当复合统计规律，即亮的区域多的就是亮场景，暗的多的就是夜晚场景。同时我们有两张输入图同时进行参考，因此我们只需要选取合适的阈值进行判断即可。

为了节约硬件开销，在没有办法做精细直方图进行判断的前提下，我们将原图分块，对于每一块归一化之后的亮度进行判断，当两张图的亮块个数均满足预设的阈值之后，我们就会判断此场景是白天场景，并通过全局变量传递给其他模块作为指导参数进行调试。

此模块优点是可以处理多光源的复杂场景，统计过程节约资源，同时利用了两张图的整体统计数据。缺点在于精度不够，判断条件单一仅仅根据平均亮度，因此在资源开销允许的情况下，有很大的提升空间。

### 数域转换模块I：linear2log

此模块是针对Combine模块的准备模块，因为combine过程实际上是归一化之后的两张图加权平均的过程，因此涉及到一些乘法操作，我们采用log域对源数据进行处理一部分的目的就在于此，可以通过log域的运算提高运算效率。其次，在log域对高比特数据进行存储运算可以有效的节约位宽。同时因为人眼对于图像亮度的感官特性决定了人眼对于暗处的变化比较敏感，而亮处的差异比较容易忽视，因此log域的亮度信息实际上是符合人眼观测特性的。综合考虑，我们在对源数据进行combine操作之前，进行了一次数域转换：从线性域依据同样的规则把全图转化为log域数据输出。

实际采用的公式如下：
```
  y = 1024 * (log2(x + 1) - 3);
```

可以看到几个关键的参数，1024只是为了扩位宽使用，进行一次线性拉伸。采用以2为底的log函数因为2是最小能使用的自然数，而且他符合我们二进制世界的要求，硬件实在太喜欢2这个数字了。而源数据进行了一次平移保证原始数据经过处理之后都可以生成一个正log域的数。而最后（-3）操作是因为我们建立了一个习惯的噪声模型，我们认为在低照度条件下，亮度小于8的值都是噪点，因此我们实际不需要对他们进行操作：没错，都写成0就好了。

在实际的算法操作过程中，我们采用了位运算的方式方便的对源数据进行了多次分段。我们利用以2为底对数的优势，直接进行位运算通过计算二进制情况下源数据的最高位1在哪里可以直接判断出源数据的取值范围，输入的各个取值节点就是2的n次方。并且我们提前可以知道各个节点的输出值（没错二进制的世界就是这么容易计算）。

在确定源数据在哪个2的幂指数范围后，我们再采用类似的方法对剩下的bit位进行第二次分段判断，我们将其分为八段，并初始值拟合一条分段的log曲线作为每一段的斜率，找到位置后便可以直接按照前节点斜率以及距离插值出我们输出的log值。

经测试，这种计算log的方式与公式直接比较误差在2以内。

### 合成模块

前文其实已经介绍过combine具体需要解决的问题，在我个人看来，combine所要解决的问题不止于此。

combine这个算法的出现主要是因为理想与现实出现了一些偏差，首先单一低位宽的数据没有办法存储高动态的实际图像，尤其对于人眼感受而言，低照度的一点点范围却承担了人类观感很大一部分的感受，因此很难直接线性映射实际的照度。同时对于sensor这样一个物理器件而言，它也是没有办法做到完全线性的映射，比如我们没有办法期待在边界亮度处，对于线性曝光增益的增长，sensor的输出响应也是线性的。尤其对于部分低质量的HDRsensor，暗处/亮处都会出现大量物理条件限制的噪点，模糊等。而对于这样的场景，暗处/亮处却正好是我们最希望能够看清的。所以在一张图没有办法做好的情况下，我们只能让步，能不能说一张图看得清暗处，让它的亮处就再糟糕一点吧，回头我用另一张图能看清的亮处拼起来就好了。所以这就构成了我们combine算法的原始思想，对于两张图取长补短，一个图的动态范围不够，那我们用两张图还不行么？（当然实际上我们两张图也是远远不够的，动态范围依然不够用）。

理想情况下，我们对于sensor器件的曝光要求应该是线性响应的，也就是说生成图的亮度比值应该跟曝光增益比完全一致。因此对我们而言，我们需要综合两者的差异，来得到理想情况下，这个照度这个曝光增益下应该得到的理想图像。实际上标定相机响应曲线的方式就是利用10+帧同一场景图像用最小二乘法拟合出来一条理想情况下的生成亮度与实际照度的非线性映射曲线来衡量此sensor原始的器件响应属性。

也就是说我们首先需要根据曝光比归一化所有的图像，再比较正常情况下在每一个照度下sensor应该输出的亮度值。我们的combine算法实际上是这一思想的简化以及参数抽象化。

#### 计算曝光比

为了归一化两张不同曝光的输入图，我们需要计算两张图的曝光比，而长短曝光均有部分效果很差的亮度分布区域，因此在统计过程中，我们对于每一帧算法处理，仅仅统计长图范围内某一段合适区间的长短图pixel亮度作为曝光比判断依据。

算法实际就是遍历输入原图，在原图像素亮度值在预设范围内进行累加并最终求解平均值，长短图亮度平均值之比作为两图曝光比输出给后续模块使用。

####






# HDR当前存在的问题

- 为什么combine模块要放到log域做，除了计算上可以把乘法转化为加法优化处理速度之外，是否有提起的作用：此问题我觉得已经解决，应该是为了节省位宽并符合人眼感官要求。

- Gamma模块添加两个是否多余：这个问题我觉得也解决了，并不多余，gamma模块有两个作用，一个是2.2gamma用来反映射CRT屏幕的响应曲线，为了找到真实的颜色，而我们的gamma是用来局部提亮为了满足视觉的真实感受需求。

# HDR后续的设想

- log域可能是因为进行压缩之后相当于一次成功的gamma操作，那么使用linear域进行该操作是否可行？

- HDR采用的多帧图像理论上进行合成应该能起到原图去噪的效果，因此可否通过combine进行合成并保持全部细节交给tonemapping进行WDR类似的操作即有HDR的效果又能降噪？可以直接使用HDR两张图资源优势也比较大。

- 现在采用简单的限制对比度的直方图均衡方式是否足够满足需求？实际进行的操作与tonemapping本质并无相关，只是做了简单的移位，是否有可能后续修改？
